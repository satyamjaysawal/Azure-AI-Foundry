{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41474fce",
   "metadata": {},
   "source": [
    "## Azure OpenAI GPT Inferencing with APIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64404b53",
   "metadata": {},
   "source": [
    "![Description of GIF](./Assets/GPT-4o-inferencing.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec60030",
   "metadata": {},
   "source": [
    "### ðŸ§ª Test the API using a direct HTTP call\n",
    "\n",
    "Use the code below to send a test request to your Azure OpenAI endpoint and inspect the response.  \n",
    "This helps verify connectivity, authentication, and basic functionality before integrating into larger workflows.\n",
    "\n",
    "- **API Key:** ðŸ”‘ `apim_subscription_key`\n",
    "- **Endpoint:** ðŸŒ `url`\n",
    "- **Sample Request:**  \n",
    "    - Role-based messages for the GPT model\n",
    "\n",
    "> ðŸ“ **Tip:** Check the response headers and status code for troubleshooting and rate limit information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time \n",
    "import requests\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "runs = 1\n",
    "sleep_time_ms = 1000\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv(\"APIM_ENDPOINT\")\n",
    "apim_subscription_key = os.getenv(\"APIM_SUBSCRIPTION_KEY\")\n",
    "\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"â–¶ï¸ Run: \", i+1)\n",
    "    messages={\"messages\":[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "    ]}\n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    # Print all response headers in a pretty table\n",
    "    headers_list = [(key, value) for key, value in response.headers.items()]\n",
    "    print(\"Response Headers:\")\n",
    "    print(tabulate(headers_list, headers=[\"Header\", \"Value\"], tablefmt=\"fancy_grid\"))\n",
    "    print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"ðŸ’¬ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    else:\n",
    "        print(response.text)\n",
    "    time.sleep(sleep_time_ms/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537ca5b",
   "metadata": {},
   "source": [
    "### âš¡ï¸ API Rate Limiting Demo\n",
    "\n",
    "In the next section, we intentionally exceed the API rate limit to observe how the Azure OpenAI endpoint responds.  \n",
    "You will see a `429` status code and a message indicating that the rate limit has been reached.  \n",
    "This is useful for understanding throttling behavior and planning for robust error handling in production scenarios. ðŸš¦\n",
    "\n",
    "- **API Key Used:** ðŸ”‘ `apim_subscription_key`\n",
    "- **Endpoint:** ðŸŒ `url`\n",
    "- **Sample Response Headers:** ðŸ“¨  \n",
    "    - `Content-Length`, `Content-Type`, `Retry-After`, etc.\n",
    "\n",
    "> ðŸ’¡ Tip: Always implement retry logic and respect the `Retry-After` header to avoid service disruptions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exhaust the rate limit\n",
    "import time\n",
    "for i in range(200):  # Adjust as needed to exceed your rate limit\n",
    "    messages = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers={'api-key': apim_subscription_key}, json=messages)\n",
    "    print(f\"Request {i+1}: Status {response.status_code}\")\n",
    "    if response.status_code == 429:\n",
    "        print(\"Rate limit exceeded!\")\n",
    "        print(response.json())\n",
    "        break\n",
    "    # Optional: sleep to avoid flooding too quickly\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308c807",
   "metadata": {},
   "source": [
    "### ðŸ§ª Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Use the code below to send a test request to your Azure OpenAI endpoint using the official Python SDK.  \n",
    "This approach provides a more streamlined and Pythonic way to interact with the API compared to raw HTTP requests.\n",
    "\n",
    "- **API Key:** ðŸ”‘ `apim_subscription_key`\n",
    "- **Endpoint:** ðŸŒ `apim_resource_gateway_url`\n",
    "- **Model Deployment:** ðŸ¤– `openai_model_name`\n",
    "- **API Version:** ðŸ“… `openai_api_version`\n",
    "- **Sample Request:**  \n",
    "    - Role-based messages for the GPT model\n",
    "\n",
    "> ðŸ“ **Tip:** The SDK handles authentication, serialization, and error handling for you, making integration easier and more robust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "runs = 1\n",
    "sleep_time_ms = 1000\n",
    "\n",
    "apim_resource_gateway_url = os.getenv(\"APIM_GATEWAY_URL\")\n",
    "openai_model_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "print(\"APIM Gateway URL: \", apim_resource_gateway_url)\n",
    "print(\"OpenAI Model Name: \", openai_model_name)\n",
    "print(\"OpenAI API Version: \", openai_api_version)\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"â–¶ï¸ Run: \", i+1)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "    ]\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    response = client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    print(\"ðŸ’¬ \", response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829b6a0",
   "metadata": {},
   "source": [
    "### ðŸ“ Image Processing\n",
    "\n",
    "In this section, we use the Azure OpenAI Python SDK to generate a response from the GPT model and render it directly as Markdown in the notebook.\n",
    "\n",
    "- **API Endpoint:** ðŸŒ `apim_resource_gateway_url`\n",
    "- **Model:** ðŸ¤– `openai_model_name`\n",
    "- **API Version:** ðŸ“… `openai_api_version`\n",
    "- **Client Instance:** ðŸ”‘ `client`\n",
    "\n",
    "The code below demonstrates how to send a prompt and display the assistant's answer with rich formatting using `IPython.display.Markdown`. This is especially useful for math, code, and structured content!\n",
    "\n",
    "Below, the referenced image is also displayed for context:\n",
    "\n",
    "![Triangle Diagram](https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png)\n",
    "\n",
    "> ðŸ’¡ **Tip:** Rendering responses as Markdown makes explanations, formulas, and code snippets easier to read and visually appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b661532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "response = client.chat.completions.create(\n",
    "    model=openai_model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637fb49",
   "metadata": {},
   "source": [
    "### ðŸ–¼ï¸ Base64 Image Processing Demo\n",
    "\n",
    "In this section, we'll demonstrate how to send a local image (`meme.png`) as a base64-encoded string to the Azure OpenAI API for analysis. This is useful for scenarios where you want to process images that aren't hosted online.\n",
    "\n",
    "- **Image Used:** `meme.png`\n",
    "- **Encoding:** Base64\n",
    "- **API:** Azure OpenAI GPT-4o\n",
    "- **Response:** The assistant interprets the meme and explains the humor!\n",
    "\n",
    "> ðŸ’¡ **Tip:** Base64 encoding allows you to transmit image data directly in your API requests, making it easy to work with local files in notebooks.\n",
    "\n",
    "![Meme Example](meme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f143dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "import base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "base64_image = encode_image(\"meme.png\")\n",
    "response = client.chat.completions.create(\n",
    "    model=openai_model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps to interprete memes!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's funny about this picture?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "HTML(f'<font size=\"5\">{response.choices[0].message.content}</font>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
