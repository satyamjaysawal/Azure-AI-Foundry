{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b3e033",
   "metadata": {},
   "source": [
    "## Introduction to Azure Redis Cache for Semantic Search Capabilities üß†üîé\n",
    "\n",
    "Unlock the power of **semantic caching** by integrating Azure Redis Cache with OpenAI models! In this lab, you'll learn how to set up semantic caching for LLM responses, reducing latency and cost for repeated or similar queries.\n",
    "\n",
    "---\n",
    "\n",
    "![Azure Redis Cache Semantic Search](redis-semantic-caching-scaled.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5a691",
   "metadata": {},
   "source": [
    "### üöÄ Install Required Packages\n",
    "\n",
    "Let's start by installing all the Python packages needed for this lab:  \n",
    "- `openai` for LLM access  \n",
    "- `langchain` for LLM orchestration  \n",
    "- `redis` for cache connectivity  \n",
    "- `tiktoken` for tokenization  \n",
    "- `python-dotenv` for environment variable management\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7363bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain redis tiktoken python-dotenv langchain-openai redis==4.5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c56ae",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Load Environment & Initialize Variables\n",
    "\n",
    "Load your environment variables and set up all the configuration needed to connect to Azure OpenAI and Redis.  \n",
    "This includes API keys, deployment names, and Redis connection info.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379993c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import redis\n",
    "import os\n",
    "import langchain\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import RedisSemanticCache\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "load_dotenv()\n",
    "api_version=\"2023-05-15\"\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "llm_deployment_name = os.getenv(\"LLM_DEPLOYMENT_NAME\")\n",
    "llm_model_name = os.getenv(\"LLM_MODEL_NAME\")\n",
    "embeddings_deployment_name = os.getenv(\"EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "embeddings_model_name = os.getenv(\"EMBEDDINGS_MODEL_NAME\")\n",
    "\n",
    "print(f\"Azure OpenAI Endpoint: {azure_openai_endpoint}\")\n",
    "print(azure_openai_api_key)\n",
    "print(f\"LLM Deployment Name: {llm_deployment_name}\")\n",
    "print(f\"Embeddings Deployment Name: {embeddings_deployment_name}\")\n",
    "\n",
    "redis_endpoint = os.getenv(\"REDIS_ENDPOINT\")\n",
    "redis_password = os.getenv(\"REDIS_PASSWORD\")\n",
    "print(f\"Redis Endpoint: {redis_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fd38a",
   "metadata": {},
   "source": [
    "### ü§ñ Initialize LLM & Embeddings\n",
    "\n",
    "Create your Azure OpenAI LLM and Embeddings objects using the loaded configuration.  \n",
    "These will be used for generating responses and semantic similarity calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=azure_openai_api_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=llm_deployment_name,\n",
    "    model=llm_model_name,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=azure_openai_api_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=api_version,\n",
    "    azure_deployment=embeddings_deployment_name,\n",
    "    model=embeddings_model_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b8bff",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Connect to Redis & Set Up Semantic Cache\n",
    "\n",
    "Build the Redis connection URL and configure LangChain to use Redis as a semantic cache.  \n",
    "This enables fast retrieval of similar responses and reduces redundant LLM calls.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.cache import RedisSemanticCache\n",
    "\n",
    "# Example for Azure Redis (with SSL enabled)\n",
    "redis_url = f\"rediss://:{redis_password}@{redis_endpoint}\"  # SSL port\n",
    "\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=redis_url,\n",
    "    embedding=embeddings,\n",
    "    score_threshold=0.05,\n",
    ")\n",
    "\n",
    "set_llm_cache(semantic_cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459b904",
   "metadata": {},
   "source": [
    "### üìù Generate a Poem About Cute Kittens (First Query)\n",
    "\n",
    "Let's generate a poem about cute kittens!  \n",
    "This first call will go to the LLM and store the result in the semantic cache.\n",
    "When you run the second call, you'll see faster response times\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import redis\n",
    "response = llm(\"Please write a poem about cute kittens.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e543aab",
   "metadata": {},
   "source": [
    "### üìù Generate a Poem About Cute Puppies (First Query)\n",
    "\n",
    "Now, generate a poem about cute puppies.  \n",
    "Since this is a new query but semantically similar to the first query, this will go to the cache\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057068af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = llm(\"Please write a poem about cute puppies.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = llm(\"Please write a poem about pets\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
